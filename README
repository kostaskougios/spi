
This is a sample scala/spark project. It contains simple examples and also a couple of a bit more complex processing jobs.

For a good spark example, please see the wikipedia project, it contains code to load wikipedia exports (xml files from https://dumps.wikimedia.org/backup-index.html)
to hdfs and then process them on hadoop and export them into HBase tables (sql, phoenix).

I always trying to use the latest hadoop and spark build for it. To actually build/run this project, please build spark first as per the
instructions in this file.

-------------------------------
1. BUILDING SPARK
-------------------------------

Clone the version you need:

git clone git://github.com/apache/spark.git -b branch-2.3

And build it for the hadoop version you are using:

export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"

# Set the local artifact version
mvn versions:set -DnewVersion=2.3.0-akt

./build/mvn -DskipTests -Pyarn -Phadoop-2.7 -Dhadoop.version=2.9.0 clean package


see http://spark.apache.org/docs/latest/building-spark.html

-------------------------------
2. Compiling this project
-------------------------------

You can run those locally within your ide or build them and execute them at your hadoop cluster. To build them do:

bin/build-project wikipedia

This will create a fat-jar with the wikipedia code. Then:

bin/wikipedia-ingest-job

to run IngestWikipediaJob.

I submit these jobs to my home hadoop cluster (hadoop, hbase, zookeeper, kafka) of 3x virtual servers running on a 16 core opteron - 64GB RAM server.
I manually did the installation of hadoop by downloading the tar.gz files and installing them on each virtual server.


Other examples include:

- sample hbase job in hbase project, FillTableJob.

- sample kafka/spark streaming code in kafka project

- sample hbase/phoenix code in phoenix / SamplePopulateJob

- spark sql (TODO)

