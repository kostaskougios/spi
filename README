
This is a sample scala/spark project. It contains simple examples and also a couple of a bit more complex processing jobs.

For a good spark example, please see the wikipedia project, it contains code to load wikipedia exports (xml files from https://dumps.wikimedia.org/backup-index.html)
to hdfs and then process them on hadoop and export them into HBase tables (sql, phoenix).

I always trying to use the latest hadoop and spark build for it. To actually build/run this project, please build spark first as per the
instructions in this file.

Steps to run the wikipedia code to process wikipedia xml files and copy them to hbase/phoenix tables:

- build spark (see instructions below)
- Download the wikipedia exports from https://dumps.wikimedia.org/backup-index.html . The EN language is the largest export, the EL language is small and can help for quicker tests.
- The loaders project contains BreakupBigXmlFilesAndStoreToHdfs class which can be used to efficiently upload the xml files to hdfs for efficient spark processing. Run it
with the JVM args as per the class comments.
- Now you should have a /wikipedia folder in hdfs.
- Run

bin/build-project wikipedia

to build the wikipedia fat jar which will contain all the spark jobs for wikipedia processing.

- Inspect & run

bin/wikipedia-ingest-job

to run the 1st job that processes the wikipedia data to an easier to use format.

- If you want to populate hbase/phoenix tables, create the tables as per wikipedia/ddl.sql and then run

bin/wikipedia-populate-database-job
bin/wikipedia-words-job

Those jobs can run on the same time.

-------------------------------
1. BUILDING SPARK
-------------------------------

Clone the version you need:

git clone git://github.com/apache/spark.git -b branch-2.3

And build it for the hadoop version you are using:

export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"

# Set the local artifact version
mvn versions:set -DnewVersion=2.3.0-hadoop-2.7.6-akt

./build/mvn -DskipTests -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.6 clean install


see http://spark.apache.org/docs/latest/building-spark.html

-------------------------------
2. Compiling this project
-------------------------------

You can run those locally within your ide or build them and execute them at your hadoop cluster. To build them do:

bin/build-project wikipedia

This will create a fat-jar with the wikipedia code. Then:

bin/wikipedia-ingest-job

to run IngestWikipediaJob.

I submit these jobs to my home hadoop cluster (hadoop, hbase, zookeeper, kafka) of 3x virtual servers running on a 16 core opteron - 64GB RAM server.
The installation of all servers is described in my linkedin account, https://www.linkedin.com/in/kostaskougios/


Other examples include:

- sample hbase job in hbase project, FillTableJob.

- sample kafka/spark streaming code in kafka project

- sample hbase/phoenix code in phoenix / SamplePopulateJob

- spark sql (TODO)

